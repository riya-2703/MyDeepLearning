{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "McCulloch pit AND Gate"
      ],
      "metadata": {
        "id": "2aCN-FD_bXa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTC2YremasyK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_and = np.array([\n",
        "    [0,0],\n",
        "    [0,1],\n",
        "    [1,0],\n",
        "    [1,1]\n",
        "])\n",
        "\n",
        "w_and = np.array([1,1])\n",
        "\n",
        "prod_and = input_and @ w_and\n",
        "print(f'Product: {prod_and}')\n",
        "\n",
        "def step_function(p:int, threshold:int) -> bool:\n",
        "  if p>= threshold:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "threshold_and = 2\n",
        "for i in range(0,4):\n",
        "  activate = step_function(prod_and[i], threshold_and)\n",
        "  if(activate):\n",
        "    print(f' Activation: 1')\n",
        "  else:\n",
        "    print(f' Activation: 0')\n",
        "\n",
        "activations_and = [step_function(p,threshold_and) for p in prod_and]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(input[:, 0], input[:, 1], c=activations_and, cmap='viridis', s=100)\n",
        "\n",
        "x = np.linspace(-0.5, 1.5, 100)\n",
        "y = 0.7 * threshold_and - w_and[0] * x\n",
        "plt.plot(x, y, '-r', label='Decision Boundary')\n",
        "\n",
        "plt.scatter(input[:, 0][activations_and], input[:, 1][activations_and], c='green', marker='o', s=200, label='Activated')\n",
        "\n",
        "plt.title('AND Gate Decision Boundary')\n",
        "plt.xlabel('Input 1')\n",
        "plt.ylabel('Input 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "McCulloch Pit OR Gate"
      ],
      "metadata": {
        "id": "AtjAQqHmeY8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_or = np.array([\n",
        "    [0,0],\n",
        "    [0,1],\n",
        "    [1,0],\n",
        "    [1,1]\n",
        "])\n",
        "\n",
        "w_or = np.array([1,1])\n",
        "\n",
        "prod_or = input_or @ w_or\n",
        "print(f'Product : {prod_or}')\n",
        "\n",
        "def step_function(p:int, threshold:int) -> bool:\n",
        "  if p>= threshold:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "threshold_or = 1\n",
        "\n",
        "for i in range(0,4):\n",
        "  activate = step_function(prod_or[i], threshold_or)\n",
        "  if(activate):\n",
        "    print(f' Activation: 1')\n",
        "  else:\n",
        "    print(f' Activation: 0')\n",
        "\n",
        "activations_or = [step_function(p, threshold_or) for p in prod_or]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(input[:, 0], input[:,1], c=activations_or, cmap='viridis', s=100)\n",
        "\n",
        "x = np.linspace(-0.5, 1.5, 100)\n",
        "y = 0.5*threshold_or - w_or[0] * x\n",
        "plt.plot(x, y, '-r', label='Decision Boundary')\n",
        "\n",
        "plt.scatter(input[:, 0][activations_or], input[:, 1][activations_or], c='green', marker='o', s=200, label='Activated')\n",
        "\n",
        "plt.title('OR Gate Decision Boundary')\n",
        "plt.xlabel('Input 1')\n",
        "plt.ylabel('Input 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OEA9e2m5ecAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "McCulloch Pitt NAND Gate"
      ],
      "metadata": {
        "id": "T7-i6o_shPUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_nand = np.array([\n",
        "    [0,0],\n",
        "    [0,1],\n",
        "    [1,0],\n",
        "    [1,1]\n",
        "])\n",
        "\n",
        "w_nand = np.array([-1,-1])\n",
        "\n",
        "prod_nand = input_nand @ w_nand\n",
        "print(f' Product: {prod_nand}')\n",
        "\n",
        "def step_function(p:int, threshold:int) -> bool:\n",
        "  if p>=threshold:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "threshold_nand = -1\n",
        "\n",
        "for i in range(0,4):\n",
        "  activate = step_function(prod_nand[i], threshold_nand)\n",
        "  if(activate):\n",
        "    print(' Activation: 1')\n",
        "  else:\n",
        "    print(f' Activation: 0')\n",
        "\n",
        "activations_nand = [step_function(p, threshold_nand) for p in prod_nand]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(input[:,0], input[:,1], c=activations_nand, cmap=\"viridis\", s=100)\n",
        "\n",
        "x = np.linspace(-0.5, 2, 100)\n",
        "y = -x + 1.5\n",
        "plt.plot(x, y, '-r', label='Decision Boundary')\n",
        "\n",
        "plt.scatter(input[:, 0][activations_nand], input[:, 1][activations_nand], c='green', marker='o', s=200, label='Activated')\n",
        "\n",
        "plt.title('NAND Gate Decision Boundary')\n",
        "plt.xlabel('Input 1')\n",
        "plt.ylabel('Input 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BtKqhHRahST9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design and implement a fully connected deep neural network with at least 2 hidden layers for a classification application. Use appropriate Learning Algorithm, output function, and loss function. (MNIST dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "ekU1bOSWl6Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train.reshape(-1, 28*28)).reshape(-1, 28, 28)\n",
        "x_test = scaler.transform(x_test.reshape(-1, 28*28)).reshape(-1, 28, 28)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val), verbose=1)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    index = np.random.randint(0, len(x_test))\n",
        "    image = x_test[index]\n",
        "    label = y_test[index]\n",
        "\n",
        "    prediction = np.argmax(model.predict(image.reshape(1, 28, 28)))\n",
        "\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f\"Actual: {label}, Predicted: {prediction}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IRNMCeouoGuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design the architecture and implement the autoencoder model for Image Compression.(MNIST dataset)\n"
      ],
      "metadata": {
        "id": "Ndx4P2GLrLPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import layers, losses\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "\n",
        "# Loading the MNIST dataset and extracting training and testing data\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalizing pixel values to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Displaying the shapes of the training and testing datasets\n",
        "print(\"Shape of the training data:\", x_train.shape)\n",
        "print(\"Shape of the testing data:\", x_test.shape)\n",
        "\n",
        "# Definition of the Autoencoder model as a subclass of the TensorFlow Model class\n",
        "\n",
        "class SimpleAutoencoder(Model):\n",
        "\tdef __init__(self,latent_dimensions , data_shape):\n",
        "\t\tsuper(SimpleAutoencoder, self).__init__()\n",
        "\t\tself.latent_dimensions = latent_dimensions\n",
        "\t\tself.data_shape = data_shape\n",
        "\n",
        "\t\t# Encoder architecture using a Sequential model\n",
        "\t\tself.encoder = tf.keras.Sequential([\n",
        "\t\t\tlayers.Flatten(),\n",
        "\t\t\tlayers.Dense(latent_dimensions, activation='relu'),\n",
        "\t\t])\n",
        "\n",
        "\t\t# Decoder architecture using another Sequential model\n",
        "\t\tself.decoder = tf.keras.Sequential([\n",
        "\t\t\tlayers.Dense(tf.math.reduce_prod(data_shape), activation='sigmoid'),\n",
        "\t\t\tlayers.Reshape(data_shape)\n",
        "\t\t])\n",
        "\n",
        "\t# Forward pass method defining the encoding and decoding steps\n",
        "\tdef call(self, input_data):\n",
        "\t\tencoded_data = self.encoder(input_data)\n",
        "\t\tdecoded_data = self.decoder(encoded_data)\n",
        "\t\treturn decoded_data\n",
        "\n",
        "# Extracting shape information from the testing dataset\n",
        "input_data_shape = x_test.shape[1:]\n",
        "\n",
        "# Specifying the dimensionality of the latent space\n",
        "latent_dimensions = 64\n",
        "\n",
        "# Creating an instance of the SimpleAutoencoder model\n",
        "simple_autoencoder = SimpleAutoencoder(latent_dimensions, input_data_shape)\n",
        "\n",
        "# Compile and Fit Autoencoder\n",
        "simple_autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "simple_autoencoder.fit(x_train, x_train,\n",
        "\t\t\t\tepochs=5,\n",
        "\t\t\t\tshuffle=True,\n",
        "\t\t\t\tvalidation_data=(x_test, x_test))\n",
        "\n",
        "# visualize the original and reconstructed data\n",
        "encoded_imgs = simple_autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = simple_autoencoder.decoder(encoded_imgs).numpy()\n",
        "\n",
        "n = 6\n",
        "plt.figure(figsize=(8, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(\"reconstruct\")\n",
        "  plt.gray()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "muNPtEXwrM0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design the architecture and implement the autoencoder model for Image denoising."
      ],
      "metadata": {
        "id": "CX64V16MsGsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load dataset in numpy format\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}')\n",
        "print(f'X_test.shape: {X_test.shape}')\n",
        "\n",
        "# plot images as grey scale images\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "# Formatting data\n",
        "\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}')\n",
        "print(f'X_test.shape: {X_test.shape}')\n",
        "\n",
        "# Adding noise to images\n",
        "\n",
        "noise_factor = 0.2\n",
        "x_train_noisy = X_train + noise_factor * numpy.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
        "x_test_noisy = X_test + noise_factor * numpy.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
        "x_train_noisy = numpy.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = numpy.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Defining an encoder - decoder network\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=num_pixels, activation='relu'))\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(784, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Training model\n",
        "model.fit(x_train_noisy, X_train, validation_data=(x_test_noisy, X_test), epochs=2, batch_size=200)\n",
        "\n",
        "# Evaluation of model\n",
        "\n",
        "pred = model.predict(x_test_noisy)\n",
        "print(f'pred.shape : {pred.shape}')\n",
        "\n",
        "print(f'X_test.shape: {X_test.shape}')\n",
        "\n",
        "X_test = numpy.reshape(X_test, (10000,28,28)) *255\n",
        "pred = numpy.reshape(pred, (10000,28,28)) *255\n",
        "x_test_noisy = numpy.reshape(x_test_noisy, (-1,28,28)) *255\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Test Images\")\n",
        "for i in range(10,20,1):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(X_test[i,:,:], cmap='gray')\n",
        "    curr_lbl = y_test[i]\n",
        "    plt.title(\"(Label: \" + str(curr_lbl) + \")\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Test Images with Noise\")\n",
        "for i in range(10,20,1):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(x_test_noisy[i,:,:], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Reconstruction of Noisy Test Images\")\n",
        "for i in range(10,20,1):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(pred[i,:,:], cmap='gray')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "iM1EgzwssGWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a backpropagation algorithm to train a DNN with at least 2 hidden layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "QHhrBFJ5cxCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size1 = hidden_size1\n",
        "    self.hidden_size2 = hidden_size2\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.weights_input_hidden1 = np.random.randn(self.input_size, self.hidden_size1)\n",
        "    self.bias_hidden1 = np.random.randn(1, self.hidden_size1)\n",
        "    self.weights_hidden1_hidden2 = np.random.randn(self.hidden_size1, self.hidden_size2)\n",
        "    self.bias_hidden2 = np.random.randn(1, self.hidden_size2)\n",
        "    self.weights_hidden2_output = np.random.randn(self.hidden_size2, self.output_size)\n",
        "    self.bias_output = np.random.randn(1, self.output_size)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "  def feedforward(self, X):\n",
        "    self.hidden_output1 = self.sigmoid(np.dot(X, self.weights_input_hidden1) + self.bias_hidden1)\n",
        "    self.hidden_output2 = self.sigmoid(np.dot(self.hidden_output1, self.weights_hidden1_hidden2) + self.bias_hidden2)\n",
        "    self.output = self.sigmoid(np.dot(self.hidden_output2, self.weights_hidden2_output) + self.bias_output)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, X, y, output, learning_rate):\n",
        "    # Calculate error\n",
        "    output_error = y - output\n",
        "\n",
        "    # Compute gradients for output layer\n",
        "    output_delta = output_error * self.sigmoid_derivative(output)\n",
        "\n",
        "    # Compute gradients for hidden layer 2\n",
        "    hidden2_error = output_delta.dot(self.weights_hidden2_output.T)\n",
        "    hidden2_delta = hidden2_error * self.sigmoid_derivative(self.hidden_output2)\n",
        "\n",
        "    # Compute gradients for hidden layer 1\n",
        "    hidden1_error = hidden2_delta.dot(self.weights_hidden1_hidden2.T)\n",
        "    hidden1_delta = hidden1_error * self.sigmoid_derivative(self.hidden_output1)\n",
        "\n",
        "    # Update weights and biases\n",
        "    self.weights_hidden2_output += self.hidden_output2.T.dot(output_delta) * learning_rate\n",
        "    self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "    self.weights_hidden1_hidden2 += self.hidden_output1.T.dot(hidden2_delta) * learning_rate\n",
        "    self.bias_hidden2 += np.sum(hidden2_delta, axis=0, keepdims=True) * learning_rate\n",
        "    self.weights_input_hidden1 += X.T.dot(hidden1_delta) * learning_rate\n",
        "    self.bias_hidden1 += np.sum(hidden1_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "  def train(self, X, y, epochs, learning_rate):\n",
        "    for epoch in range(epochs):\n",
        "      output = self.feedforward(X)\n",
        "      self.backward(X, y, output, learning_rate)\n",
        "      if epoch % 100 == 0:\n",
        "        loss = np.mean(np.square(y - output))\n",
        "        print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(y.reshape(-1,1)).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 4\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
        "nn.train(X_train, y_train, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "def predict_classes(output):\n",
        "    return np.argmax(output, axis=1)\n",
        "\n",
        "test_output = nn.feedforward(X_test)\n",
        "predicted_classes = predict_classes(test_output)\n",
        "true_classes = predict_classes(y_test)\n",
        "\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "RmT2m50Ccyyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent, Mini Batch Gradient Descent, Adagrad GD, Adam\n",
        " Learning GD"
      ],
      "metadata": {
        "id": "BWkncQJiidHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SingleLayerNN:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size)\n",
        "        self.bias = np.random.randn(output_size)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "\n",
        "    def stochastic_gradient_descent(self, X, y, learning_rate=0.01):\n",
        "        losses = []\n",
        "        for _ in range(100):  # Number of epochs\n",
        "            total_loss = 0\n",
        "            for i in range(X.shape[0]):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = prediction - y[i]\n",
        "                self.weights -= learning_rate * np.outer(X[i], error)\n",
        "                self.bias -= learning_rate * error\n",
        "                total_loss += np.sum(error ** 2)\n",
        "            losses.append(total_loss)\n",
        "        return losses\n",
        "\n",
        "    def mini_batch_gradient_descent(self, X, y, batch_size=32, learning_rate=0.01):\n",
        "        losses = []\n",
        "        for _ in range(100):  # Number of epochs\n",
        "            total_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                prediction = self.predict(X_batch)\n",
        "                error = prediction - y_batch\n",
        "                self.weights -= learning_rate * np.dot(X_batch.T, error) / batch_size\n",
        "                self.bias -= learning_rate * np.mean(error, axis=0)\n",
        "                total_loss += np.sum(error ** 2)\n",
        "            losses.append(total_loss)\n",
        "        return losses\n",
        "\n",
        "\n",
        "    def momentum_gradient_descent(self, X, y, learning_rate=0.01, momentum=0.9):\n",
        "        velocity_weights = np.zeros_like(self.weights)\n",
        "        velocity_bias = np.zeros_like(self.bias)\n",
        "        losses = []\n",
        "        for _ in range(100):  # Number of epochs\n",
        "            total_loss = 0\n",
        "            for i in range(X.shape[0]):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = prediction - y[i]\n",
        "                grad_weights = np.outer(X[i], error)\n",
        "                grad_bias = error\n",
        "                velocity_weights = momentum * velocity_weights - learning_rate * grad_weights\n",
        "                velocity_bias = momentum * velocity_bias - learning_rate * grad_bias\n",
        "                self.weights += velocity_weights\n",
        "                self.bias += velocity_bias\n",
        "                total_loss += np.sum(error ** 2)\n",
        "            losses.append(total_loss)\n",
        "        return losses\n",
        "\n",
        "    def nestorev_gradient_descent(self, X, y, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        m_weights = np.zeros_like(self.weights)\n",
        "        v_weights = np.zeros_like(self.weights)\n",
        "        m_bias = np.zeros_like(self.bias)\n",
        "        v_bias = np.zeros_like(self.bias)\n",
        "        losses = []\n",
        "        for _ in range(100):  # Number of epochs\n",
        "            total_loss = 0\n",
        "            for i in range(X.shape[0]):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = prediction - y[i]\n",
        "                grad_weights = np.outer(X[i], error)\n",
        "                grad_bias = error\n",
        "                m_weights = beta1 * m_weights + (1 - beta1) * grad_weights\n",
        "                v_weights = beta2 * v_weights + (1 - beta2) * (grad_weights ** 2)\n",
        "                m_bias = beta1 * m_bias + (1 - beta1) * grad_bias\n",
        "                v_bias = beta2 * v_bias + (1 - beta2) * (grad_bias ** 2)\n",
        "                m_weights_hat = m_weights / (1 - beta1)\n",
        "                v_weights_hat = v_weights / (1 - beta2)\n",
        "                m_bias_hat = m_bias / (1 - beta1)\n",
        "                v_bias_hat = v_bias / (1 - beta2)\n",
        "                self.weights -= learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "                self.bias -= learning_rate * m_bias_hat / (np.sqrt(v_bias_hat) + epsilon)\n",
        "                total_loss += np.sum(error ** 2)\n",
        "            losses.append(total_loss)\n",
        "        return losses\n",
        "\n",
        "    def adagrad_gradient_descent(self, X, y, learning_rate=0.01, epsilon=1e-8):\n",
        "        grad_weights_squared = np.zeros_like(self.weights)\n",
        "        grad_bias_squared = np.zeros_like(self.bias)\n",
        "        losses = []\n",
        "        for _ in range(100):  # Number of epochs\n",
        "            total_loss = 0\n",
        "            for i in range(X.shape[0]):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = prediction - y[i]\n",
        "                grad_weights = np.outer(X[i], error)\n",
        "                grad_bias = error\n",
        "                grad_weights_squared += grad_weights ** 2\n",
        "                grad_bias_squared += grad_bias ** 2\n",
        "                self.weights -= learning_rate * grad_weights / (np.sqrt(grad_weights_squared) + epsilon)\n",
        "                self.bias -= learning_rate * grad_bias / (np.sqrt(grad_bias_squared) + epsilon)\n",
        "                total_loss += np.sum(error ** 2)\n",
        "            losses.append(total_loss)\n",
        "        return losses\n",
        "\n",
        "    def adam_learning_gradient_descent(self, X, y, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        m_weights = np.zeros_like(self.weights)\n",
        "        v_weights = np.zeros_like(self.weights)\n",
        "        m_bias = np.zeros_like(self.bias)\n",
        "        v_bias = np.zeros_like(self.bias)\n",
        "        t = 0\n",
        "        losses = []\n",
        "        for _ in range(100):  # Number of epochs\n",
        "            total_loss = 0\n",
        "            for i in range(X.shape[0]):\n",
        "                t += 1\n",
        "                prediction = self.predict(X[i])\n",
        "                error = prediction - y[i]\n",
        "                grad_weights = np.outer(X[i], error)\n",
        "                grad_bias = error\n",
        "                m_weights = beta1 * m_weights + (1 - beta1) * grad_weights\n",
        "                v_weights = beta2 * v_weights + (1 - beta2) * (grad_weights ** 2)\n",
        "                m_bias = beta1 * m_bias + (1 - beta1) * grad_bias\n",
        "                v_bias = beta2 * v_bias + (1 - beta2) * (grad_bias ** 2)\n",
        "                m_weights_hat = m_weights / (1 - beta1 ** t)\n",
        "                v_weights_hat = v_weights / (1 - beta2 ** t)\n",
        "                m_bias_hat = m_bias / (1 - beta1 ** t)\n",
        "                v_bias_hat = v_bias / (1 - beta2 ** t)\n",
        "                self.weights -= learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "                self.bias -= learning_rate * m_bias_hat / (np.sqrt(v_bias_hat) + epsilon)\n",
        "                total_loss += np.sum(error ** 2)\n",
        "            losses.append(total_loss)\n",
        "        return losses\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = len(np.unique(y))\n",
        "y_one_hot = tf.one_hot(y, depth=num_classes).numpy()\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize model\n",
        "model = SingleLayerNN(input_size=X_train.shape[1], output_size=num_classes)\n",
        "\n",
        "# Train using different optimization algorithms\n",
        "sgd_losses = model.stochastic_gradient_descent(X_train, y_train)\n",
        "mbgd_losses = model.mini_batch_gradient_descent(X_train, y_train)\n",
        "mgd_losses = model.momentum_gradient_descent(X_train, y_train)\n",
        "nestorev_losses = model.nestorev_gradient_descent(X_train, y_train)\n",
        "adagrad_losses = model.adagrad_gradient_descent(X_train, y_train)\n",
        "adam_losses = model.adam_learning_gradient_descent(X_train, y_train)\n",
        "\n",
        "# Plot losses for each optimization algorithm\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(sgd_losses, label='Stochastic GD')\n",
        "plt.plot(mbgd_losses, label='Mini Batch GD')\n",
        "plt.plot(mgd_losses, label='Momentum GD')\n",
        "plt.plot(nestorev_losses, label='Nestorev GD')\n",
        "plt.plot(adagrad_losses, label='Adagrad GD')\n",
        "plt.plot(adam_losses, label='Adam Learning GD')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epochs for Different Gradient Descent Methods')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set using each optimization algorithm\n",
        "test_loss_sgd = model.stochastic_gradient_descent(X_test, y_test)\n",
        "test_loss_mbgd = model.mini_batch_gradient_descent(X_test, y_test)\n",
        "test_loss_mgd = model.momentum_gradient_descent(X_test, y_test)\n",
        "test_loss_nestorev = model.nestorev_gradient_descent(X_test, y_test)\n",
        "test_loss_adagrad = model.adagrad_gradient_descent(X_test, y_test)\n",
        "test_loss_adam = model.adam_learning_gradient_descent(X_test, y_test)\n",
        "\n",
        "print(\"Test Loss using Stochastic Gradient Descent:\", test_loss_sgd[-1])\n",
        "print(\"Test Loss using Mini Batch Gradient Descent:\", test_loss_mbgd[-1])\n",
        "print(\"Test Loss using Momentum Gradient Descent:\", test_loss_mgd[-1])\n",
        "print(\"Test Loss using Nestorev Gradient Descent:\", test_loss_nestorev[-1])\n",
        "print(\"Test Loss using Adagrad Gradient Descent:\", test_loss_adagrad[-1])\n",
        "print(\"Test Loss using Adam Learning Gradient Descent:\", test_loss_adam[-1])"
      ],
      "metadata": {
        "id": "sCl8M3cnilLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement 2 input Multilayer Perceptron algorithm to simulate AND gate.\n",
        " Plot the decision boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "TyW_BwMdoG-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.ones(num_features)\n",
        "        self.bias = 0.3\n",
        "        print(f\"Initial Weights: {self.weights}, Initial Bias: {self.bias}\")\n",
        "        epoch = 0\n",
        "        while True:\n",
        "            misclassified = False\n",
        "            misclassified_indices = []\n",
        "            for i in range(num_samples):\n",
        "                prediction = self.predict(X[i])\n",
        "                if prediction != y[i]:\n",
        "                    self.weights += self.learning_rate * (y[i] - prediction) * X[i]\n",
        "                    self.bias += self.learning_rate * (y[i] - prediction)\n",
        "                    misclassified = True\n",
        "                    misclassified_indices.append(i)\n",
        "            print(f\"Iteration {epoch+1}: Weights: {self.weights}, Bias: {self.bias}, Misclassified Samples: {misclassified_indices}\")\n",
        "            print(\"Testing after iteration:\")\n",
        "            for i in range(len(X)):\n",
        "                prediction = self.predict(X[i])\n",
        "                print(f\"Input: {X[i]}, Predicted Output: {prediction}\")\n",
        "            if not misclassified:\n",
        "                print(\"Training converged. Stopping iterations.\")\n",
        "                break\n",
        "            epoch += 1\n",
        "\n",
        "    def predict(self, x):\n",
        "        linear_output = np.dot(x, self.weights) + self.bias\n",
        "        return 1 if linear_output > 0 else 0\n",
        "\n",
        "# Example usage for logical AND gate\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Create and train Perceptron for logical AND gate\n",
        "perceptron_and = Perceptron()\n",
        "perceptron_and.train(X, y_and)\n",
        "\n",
        "x_values = np.linspace(-0.5, 1.5, 100)\n",
        "y_values = (-perceptron_and.weights[0] * x_values - perceptron_and.bias) / perceptron_and.weights[1]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_and, cmap='bwr', edgecolors='k', label='Data Points')\n",
        "plt.plot(x_values, y_values, 'g-', label='Decision Boundary')\n",
        "plt.title('Decision Boundary for Logical AND Gate')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vORWI9ykn_QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement 3 input Multilayer Perceptron algorithm to simulate AND gate.\n",
        " Plot the decision boundary.\n"
      ],
      "metadata": {
        "id": "GEk8rADGuwVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.ones(num_features)\n",
        "        self.bias = 0.3\n",
        "        print(f\"Initial Weights: {self.weights}, Initial Bias: {self.bias}\")\n",
        "        epoch = 0\n",
        "        while True:\n",
        "            misclassified = False\n",
        "            misclassified_indices = []\n",
        "            for i in range(num_samples):\n",
        "                prediction = self.predict(X[i])\n",
        "                if prediction != y[i]:\n",
        "                    self.weights += self.learning_rate * (y[i] - prediction) * X[i]\n",
        "                    self.bias += self.learning_rate * (y[i] - prediction)\n",
        "                    misclassified = True\n",
        "                    misclassified_indices.append(i)\n",
        "            print(f\"Iteration {epoch+1}: Weights: {self.weights}, Bias: {self.bias}, Misclassified Samples: {misclassified_indices}\")\n",
        "            print(\"Testing after iteration:\")\n",
        "            for i in range(len(X)):\n",
        "                prediction = self.predict(X[i])\n",
        "                print(f\"Input: {X[i]}, Predicted Output: {prediction}\")\n",
        "            if not misclassified:\n",
        "                print(\"Training converged. Stopping iterations.\")\n",
        "                break\n",
        "            epoch += 1\n",
        "\n",
        "    def predict(self, x):\n",
        "        linear_output = np.dot(x, self.weights) + self.bias\n",
        "        return 1 if linear_output > 0 else 0\n",
        "\n",
        "# Example usage for logical AND gate\n",
        "X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
        "y_and = np.array([0,0,0,0,0,0,0,1])\n",
        "\n",
        "# Create and train Perceptron for logical AND gate\n",
        "perceptron_and = Perceptron()\n",
        "perceptron_and.train(X, y_and)\n",
        "\n",
        "# Plotting the decision boundary as a plane in the three-dimensional space\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Generate points for the plane\n",
        "x1_range = np.linspace(0, 1, 10)\n",
        "x2_range = np.linspace(0, 1, 10)\n",
        "x1_plane, x2_plane = np.meshgrid(x1_range, x2_range)\n",
        "x3_plane = -(perceptron_and.weights[0] * x1_plane + perceptron_and.weights[1] * x2_plane + perceptron_and.bias) / perceptron_and.weights[2]\n",
        "\n",
        "# Plot the points and the decision boundary\n",
        "sc = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_and, cmap='bwr', edgecolors='k', label='Data Points')\n",
        "db = ax.plot_surface(x1_plane, x2_plane, x3_plane, alpha=0.5, rstride=100, cstride=100, color='g', label='Decision Boundary')\n",
        "\n",
        "ax.set_title('Decision Boundary for Logical AND Gate')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_zlabel('x3')\n",
        "\n",
        "# # Create legends for scatter plot and surface plot\n",
        "# handles = [sc, db]\n",
        "# labels = ['Data Points', 'Decision Boundary']\n",
        "# #ax.legend(handles, labels)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B8ehofGnuzWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement 2 input Multilayer Perceptron algorithm to simulate NAND gate.\n",
        " Plot the decision boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "3_caJEqq03Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.ones(num_features)\n",
        "        self.bias = 0.3\n",
        "        print(f\"Initial Weights: {self.weights}, Initial Bias: {self.bias}\")\n",
        "        epoch = 0\n",
        "        while True:\n",
        "            misclassified = False\n",
        "            misclassified_indices = []\n",
        "            for i in range(num_samples):\n",
        "                prediction = self.predict(X[i])\n",
        "                if prediction != y[i]:\n",
        "                    self.weights += self.learning_rate * (y[i] - prediction) * X[i]\n",
        "                    self.bias += self.learning_rate * (y[i] - prediction)\n",
        "                    misclassified = True\n",
        "                    misclassified_indices.append(i)\n",
        "            print(f\"Iteration {epoch+1}: Weights: {self.weights}, Bias: {self.bias}, Misclassified Samples: {misclassified_indices}\")\n",
        "            print(\"Testing after iteration:\")\n",
        "            for i in range(len(X)):\n",
        "                prediction = self.predict(X[i])\n",
        "                print(f\"Input: {X[i]}, Predicted Output: {prediction}\")\n",
        "            if not misclassified:\n",
        "                print(\"Training converged. Stopping iterations.\")\n",
        "                break\n",
        "            epoch += 1\n",
        "\n",
        "    def predict(self, x):\n",
        "        linear_output = np.dot(x, self.weights) + self.bias\n",
        "        return 1 if linear_output > 0 else 0\n",
        "\n",
        "# Example usage for logical NAND gate\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_nand = np.array([1, 1, 1, 0])\n",
        "\n",
        "# Create and train Perceptron for logical NAND gate\n",
        "perceptron_nand = Perceptron()\n",
        "perceptron_nand.train(X, y_nand)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_nand, cmap='bwr', edgecolors='k', label='Data Points')\n",
        "\n",
        "# Plot decision boundary\n",
        "x_values = np.linspace(-0.5, 1.5, 100)\n",
        "y_values = -(perceptron_nand.weights[0] * x_values + perceptron_nand.bias - 0.1) / perceptron_nand.weights[1]\n",
        "plt.plot(x_values, y_values, 'g--', label='Decision Boundary')\n",
        "\n",
        "plt.title('Decision Boundary for Logical NAND Gate')\n",
        "plt.xlabel('Input 1')\n",
        "plt.ylabel('Input 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1BOjBrZF1Agt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAND GATE 3 inputs"
      ],
      "metadata": {
        "id": "NFu-ePT467jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Define the activation function (sigmoid)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the NAND gate inputs and outputs\n",
        "X = np.array([[0, 0, 0],\n",
        "              [0, 0, 1],\n",
        "              [0, 1, 0],\n",
        "              [0, 1, 1],\n",
        "              [1, 0, 0],\n",
        "              [1, 0, 1],\n",
        "              [1, 1, 0]])\n",
        "y = np.array([[1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# Define the number of input, hidden, and output neurons\n",
        "input_neurons = 3\n",
        "hidden_neurons = 5\n",
        "output_neurons = 1\n",
        "\n",
        "# Initialize weights randomly\n",
        "hidden_weights = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
        "output_weights = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
        "\n",
        "# Set learning rate and number of epochs\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Train the neural network\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_activation = sigmoid(np.dot(X, hidden_weights))\n",
        "    output_activation = sigmoid(np.dot(hidden_activation, output_weights))\n",
        "\n",
        "    # Backpropagation\n",
        "    output_error = y - output_activation\n",
        "    output_delta = output_error * output_activation * (1 - output_activation)\n",
        "\n",
        "    hidden_error = output_delta.dot(output_weights.T)\n",
        "    hidden_delta = hidden_error * hidden_activation * (1 - hidden_activation)\n",
        "\n",
        "    # Update weights\n",
        "    output_weights += hidden_activation.T.dot(output_delta) * learning_rate\n",
        "    hidden_weights += X.T.dot(hidden_delta) * learning_rate\n",
        "\n",
        "# Plot decision boundary in 3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "z_min, z_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
        "\n",
        "xx, yy, zz = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1),\n",
        "                         np.arange(z_min, z_max, 0.1))\n",
        "\n",
        "mesh_input = np.c_[xx.ravel(), yy.ravel(), zz.ravel()]\n",
        "hidden_activation = sigmoid(np.dot(mesh_input, hidden_weights))\n",
        "output_activation = sigmoid(np.dot(hidden_activation, output_weights))\n",
        "Z = np.round(output_activation)\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y.flatten(), s=40, edgecolor='k', label='Data points')\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title('Decision Boundary of NAND Gate')\n",
        "ax.set_xlabel('Input 1')\n",
        "ax.set_ylabel('Input 2')\n",
        "ax.set_zlabel('Input 3')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rd_ltnXi4QJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XOR 2 inputs"
      ],
      "metadata": {
        "id": "Apr7H2H957i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, learning_rate=0.1):\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_outputs = num_outputs\n",
        "        self.learning_rate = learning_rate\n",
        "        # Initialize weights and biases for the hidden layer and output layer\n",
        "        self.hidden_weights = np.random.rand(num_inputs, num_hidden)\n",
        "        self.hidden_bias = np.zeros(num_hidden)\n",
        "        self.output_weights = np.random.rand(num_hidden, num_outputs)\n",
        "        # self.output_bias = np.zeros(num_outputs)\n",
        "        self.output_bias = 0.3\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def train(self, X, y, num_epochs=10000):\n",
        "        for epoch in range(num_epochs):\n",
        "            # Forward pass\n",
        "            hidden_output = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "            output = self.sigmoid(np.dot(hidden_output, self.output_weights) + self.output_bias)\n",
        "            # Calculate loss\n",
        "            output_error = y - output\n",
        "            loss = np.mean(np.abs(output_error))\n",
        "            # Print result after every iteration\n",
        "            print(f\"Iteration {epoch+1}, Loss: {loss}, Output: {output.flatten()}\")\n",
        "            # Break if output matches target output\n",
        "            if np.all(np.round(output) == y):\n",
        "                print(f\"Training converged. Stopping iterations. Loss: {loss}\")\n",
        "                break\n",
        "            # Backpropagation\n",
        "            output_delta = output_error * self.sigmoid_derivative(output)\n",
        "            hidden_error = output_delta.dot(self.output_weights.T)\n",
        "            hidden_delta = hidden_error * self.sigmoid_derivative(hidden_output)\n",
        "            # Update weights and biases\n",
        "            self.output_weights += hidden_output.T.dot(output_delta) * self.learning_rate\n",
        "            self.output_bias += np.sum(output_delta, axis=0) * self.learning_rate\n",
        "            self.hidden_weights += X.T.dot(hidden_delta) * self.learning_rate\n",
        "            self.hidden_bias += np.sum(hidden_delta, axis=0) * self.learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        hidden_output = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "        output = self.sigmoid(np.dot(hidden_output, self.output_weights) + self.output_bias)\n",
        "        return np.round(output)\n",
        "\n",
        "# Example usage for XOR gate\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create and train MLP for XOR gate\n",
        "mlp_xor = MLP(num_inputs=2, num_hidden=2, num_outputs=1)\n",
        "mlp_xor.train(X_xor, y_xor)\n",
        "\n",
        "# Test the trained MLP\n",
        "print(\"Testing XOR gate:\")\n",
        "for i in range(len(X_xor)):\n",
        "    prediction = mlp_xor.predict(X_xor[i])\n",
        "    print(f\"Input: {X_xor[i]}, Predicted Output: {prediction}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HSTmisUP3wAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement 3 input Multilayer Perceptron algorithm to simulate XOR gate. Plot the decision boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "ix8tHypt3mkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, learning_rate=0.1):\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_outputs = num_outputs\n",
        "        self.learning_rate = learning_rate\n",
        "        # Initialize weights and biases for the hidden layer and output layer\n",
        "        self.hidden_weights = np.random.rand(num_inputs, num_hidden)\n",
        "        self.hidden_bias = np.zeros(num_hidden)\n",
        "        self.output_weights = np.random.rand(num_hidden, num_outputs)\n",
        "        self.output_bias = 0.3\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def train(self, X, y, num_epochs=10000):\n",
        "        for epoch in range(num_epochs):\n",
        "            # Forward pass\n",
        "            hidden_output = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "            output = self.sigmoid(np.dot(hidden_output, self.output_weights) + self.output_bias)\n",
        "            # Calculate loss\n",
        "            output_error = y - output\n",
        "            loss = np.mean(np.abs(output_error))\n",
        "            # Print result after every iteration\n",
        "            print(f\"Iteration {epoch+1}, Loss: {loss}, Output: {output.flatten()}\")\n",
        "            # Break if output matches target output\n",
        "            if np.all(np.round(output) == y):\n",
        "                print(f\"Training converged. Stopping iterations. Loss: {loss}\")\n",
        "                break\n",
        "            # Backpropagation\n",
        "            output_delta = output_error * self.sigmoid_derivative(output)\n",
        "            hidden_error = output_delta.dot(self.output_weights.T)\n",
        "            hidden_delta = hidden_error * self.sigmoid_derivative(hidden_output)\n",
        "            # Update weights and biases\n",
        "            self.output_weights += hidden_output.T.dot(output_delta) * self.learning_rate\n",
        "            self.output_bias += np.sum(output_delta, axis=0) * self.learning_rate\n",
        "            self.hidden_weights += X.T.dot(hidden_delta) * self.learning_rate\n",
        "            self.hidden_bias += np.sum(hidden_delta, axis=0) * self.learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        hidden_output = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "        output = self.sigmoid(np.dot(hidden_output, self.output_weights) + self.output_bias)\n",
        "        return np.round(output)\n",
        "\n",
        "# Example usage for all possible combinations of three inputs\n",
        "X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
        "              [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
        "# Target output for XOR-like operation (1 when inputs have odd number of ones, 0 otherwise)\n",
        "y = np.array([[0], [1], [1], [0], [1], [0], [0], [1]])\n",
        "\n",
        "# Create and train MLP\n",
        "mlp = MLP(num_inputs=3, num_hidden=4, num_outputs=1)\n",
        "mlp.train(X, y)\n",
        "\n",
        "# Test the trained MLP\n",
        "print(\"Testing MLP:\")\n",
        "for i in range(len(X)):\n",
        "    prediction = mlp.predict(X[i])\n",
        "    print(f\"Input: {X[i]}, Predicted Output: {prediction}\")\n"
      ],
      "metadata": {
        "id": "RdzDxAMK5NUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}